{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4cacad1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-08-09T11:09:33.605119Z",
     "iopub.status.busy": "2022-08-09T11:09:33.604266Z",
     "iopub.status.idle": "2022-08-09T11:09:41.485684Z",
     "shell.execute_reply": "2022-08-09T11:09:41.484126Z"
    },
    "papermill": {
     "duration": 7.892535,
     "end_time": "2022-08-09T11:09:41.489138",
     "exception": false,
     "start_time": "2022-08-09T11:09:33.596603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow version 2.6.4\n",
      "We will restrict TensorFlow to max 8GB GPU RAM\n",
      "then RAPIDS can use 8GB GPU RAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-09 11:09:39.272855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-09 11:09:39.377265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-09 11:09:39.378072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-09 11:09:39.387898: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-09 11:09:39.388225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-09 11:09:39.389035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-09 11:09:39.389840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-09 11:09:41.471609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-09 11:09:41.472474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-09 11:09:41.473144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-09 11:09:41.473734: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8192 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # setup env. variables for GPUs\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "print(f\"Using TensorFlow version {tf.__version__}\")\n",
    "\n",
    "# RESTRICT TENSORFLOW TO 8GB OF GPU RAM\n",
    "# SO THAT WE HAVE 8GB RAM FOR RAPIDS\n",
    "LIMIT = 8\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "print(f\"We will restrict TensorFlow to max {LIMIT}GB GPU RAM\")\n",
    "print(f\"then RAPIDS can use {(16-LIMIT)}GB GPU RAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7fd5426",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T11:09:41.501723Z",
     "iopub.status.busy": "2022-08-09T11:09:41.501156Z",
     "iopub.status.idle": "2022-08-09T11:09:41.507677Z",
     "shell.execute_reply": "2022-08-09T11:09:41.506645Z"
    },
    "papermill": {
     "duration": 0.016194,
     "end_time": "2022-08-09T11:09:41.511293",
     "exception": false,
     "start_time": "2022-08-09T11:09:41.495099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/amex-default-prediction/sample_submission.csv\n",
      "/kaggle/input/amex-default-prediction/train_data.csv\n",
      "/kaggle/input/amex-default-prediction/test_data.csv\n",
      "/kaggle/input/amex-default-prediction/train_labels.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964870a4",
   "metadata": {
    "papermill": {
     "duration": 0.004719,
     "end_time": "2022-08-09T11:09:41.521877",
     "exception": false,
     "start_time": "2022-08-09T11:09:41.517158",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Process Train Data\n",
    "The original dataset is too large to fit in main memory in one go, therefore we will process data in chunks/batches. We split train data into 10 parts and after processing it we will save it to disk. We split test data into 20 parts. This method will avoid memory errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e80d72ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T11:09:41.533370Z",
     "iopub.status.busy": "2022-08-09T11:09:41.533037Z",
     "iopub.status.idle": "2022-08-09T11:09:41.538306Z",
     "shell.execute_reply": "2022-08-09T11:09:41.537309Z"
    },
    "papermill": {
     "duration": 0.013577,
     "end_time": "2022-08-09T11:09:41.540409",
     "exception": false,
     "start_time": "2022-08-09T11:09:41.526832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# AFTER PROCESSING DATA ONCE, UPLOAD TO KAGGLE DATASET\n",
    "# THEN SET VARIABLE BELOW TO FALSE\n",
    "# AND ATTACH DATASET TO NOTEBOOK AND PUT PATH TO DATASET BELOW\n",
    "PROCESS_DATA = True\n",
    "PATH_TO_DATA = './data/'\n",
    "#PATH_TO_DATA = '../input/amex-tensorflow/data/'\n",
    "\n",
    "# AFTER TRAINING MODEL, UPLOAD TO KAGGLE DATASET\n",
    "# THEN SET VARIABLE BELOW TO FALSE\n",
    "# AND ATTACH DATASET TO NOTEBOOK AND PUT PATH TO DATASET BELOW\n",
    "TRAIN_MODEL = True\n",
    "PATH_TO_MODEL = './model/'\n",
    "#PATH_TO_MODEL = '../input/amex-data-for-transformers-and-rnns/model/'\n",
    "\n",
    "INFER_TEST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50108e3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T11:09:41.551670Z",
     "iopub.status.busy": "2022-08-09T11:09:41.551341Z",
     "iopub.status.idle": "2022-08-09T11:13:06.239373Z",
     "shell.execute_reply": "2022-08-09T11:13:06.238331Z"
    },
    "papermill": {
     "duration": 204.701681,
     "end_time": "2022-08-09T11:13:06.247082",
     "exception": false,
     "start_time": "2022-08-09T11:09:41.545401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 458913 train targets\n",
      "There are 190 train dataframe columns\n",
      "There are 458913 unique customers in train.\n"
     ]
    }
   ],
   "source": [
    "import cupy, cudf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "if PROCESS_DATA:\n",
    "    # Load Targets\n",
    "    targets = cudf.read_csv('/kaggle/input/amex-default-prediction/train_labels.csv')\n",
    "    targets['customer_ID'] = targets['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n",
    "    print(f\"There are {targets.shape[0]} train targets\")\n",
    "    \n",
    "    # Get train column names\n",
    "    train = cudf.read_csv('../input/amex-default-prediction/train_data.csv', nrows=1) # reads only one row\n",
    "    T_COLS = train.columns\n",
    "    print(f\"There are {len(T_COLS)} train dataframe columns\")\n",
    "    \n",
    "    # Get train customer names (use pandas to avoid memory error)\n",
    "    train = pd.read_csv('/kaggle/input/amex-default-prediction/train_data.csv', usecols=['customer_ID'])\n",
    "    train['customer_ID'] = train['customer_ID'].apply(lambda x: int(x[-16:],16) ).astype('int64')\n",
    "    customers = train.drop_duplicates().sort_index().values.flatten() # 1-D arrays\n",
    "    print(f\"There are {len(customers)} unique customers in train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d523b603",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T11:13:06.260780Z",
     "iopub.status.busy": "2022-08-09T11:13:06.259178Z",
     "iopub.status.idle": "2022-08-09T11:13:06.838203Z",
     "shell.execute_reply": "2022-08-09T11:13:06.836441Z"
    },
    "papermill": {
     "duration": 0.587361,
     "end_time": "2022-08-09T11:13:06.840435",
     "exception": false,
     "start_time": "2022-08-09T11:13:06.253074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will split train data into 10 separate files.\n",
      "There will be 45891 customers in each file (except the last file).\n",
      "Below are number of rows in each file:\n",
      "[553403, 552855, 554025, 554330, 552004, 552378, 552822, 553151, 553493, 552990]\n"
     ]
    }
   ],
   "source": [
    "# Calculate size of each separate file\n",
    "def get_rows(customers, train, NUM_FILES=10, verbose = ''):\n",
    "    chunk = len(customers) // NUM_FILES\n",
    "    if verbose != '':\n",
    "        print(f\"We will split {verbose} data into {NUM_FILES} separate files.\")\n",
    "        print(f\"There will be {chunk} customers in each file (except the last file).\")\n",
    "        print(\"Below are number of rows in each file:\")\n",
    "    rows = []\n",
    "    \n",
    "    for k in range(NUM_FILES):\n",
    "        if k == (NUM_FILES-1):\n",
    "            cc = customers[k*chunk:]\n",
    "        else:\n",
    "            cc = customers[k*chunk:(k+1)*chunk]\n",
    "        s = train.loc[train.customer_ID.isin(cc)].shape[0]\n",
    "        rows.append(s)\n",
    "        \n",
    "    if verbose != '':\n",
    "        print( rows )\n",
    "        \n",
    "    return rows\n",
    "\n",
    "if PROCESS_DATA:\n",
    "    NUM_FILES = 10\n",
    "    rows = get_rows(customers, train, NUM_FILES = NUM_FILES, verbose = 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdd13eb",
   "metadata": {
    "papermill": {
     "duration": 0.005015,
     "end_time": "2022-08-09T11:13:06.850772",
     "exception": false,
     "start_time": "2022-08-09T11:13:06.845757",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Pre-process and Feature Engineering\n",
    "Now we will do the following using [Rapids](https://rapids.ai/):\n",
    "* Reduces memory usage of customer_ID column by converting to int64\n",
    "* Reduces memory usage of date time column (then deletes the column).\n",
    "* We fill NANs\n",
    "* Label encodes the categorical columns\n",
    "* We reduce memory usage dtypes of columns\n",
    "* Converts every customer into a 3D array with sequence length 13 and feature length 188\n",
    "The columns have been rearanged to have the 11 categorical features first. This makes building the TensorFlow model later easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "155ed3ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T11:13:06.863706Z",
     "iopub.status.busy": "2022-08-09T11:13:06.863400Z",
     "iopub.status.idle": "2022-08-09T11:13:07.077188Z",
     "shell.execute_reply": "2022-08-09T11:13:07.076253Z"
    },
    "papermill": {
     "duration": 0.223581,
     "end_time": "2022-08-09T11:13:07.079369",
     "exception": false,
     "start_time": "2022-08-09T11:13:06.855788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_engineering(train, PAD_CUSTOMER_TO_13_ROWS = True, targets = None):\n",
    "        \n",
    "    # REDUCE STRING COLUMNS \n",
    "    # from 64 bytes to 8 bytes, and 10 bytes to 3 bytes respectively\n",
    "    train['customer_ID'] = train['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n",
    "    train.S_2 = cudf.to_datetime( train.S_2 )\n",
    "    train['year'] = (train.S_2.dt.year-2000).astype('int8')\n",
    "    train['month'] = (train.S_2.dt.month).astype('int8')\n",
    "    train['day'] = (train.S_2.dt.day).astype('int8')\n",
    "    del train['S_2']\n",
    "        \n",
    "    # LABEL ENCODE CAT COLUMNS (and reduce to 1 byte)\n",
    "    # with 0: padding, 1: nan, 2,3,4,etc: values\n",
    "    d_63_map = {'CL':2, 'CO':3, 'CR':4, 'XL':5, 'XM':6, 'XZ':7}\n",
    "    train['D_63'] = train.D_63.map(d_63_map).fillna(1).astype('int8')\n",
    "\n",
    "    d_64_map = {'-1':2,'O':3, 'R':4, 'U':5}\n",
    "    train['D_64'] = train.D_64.map(d_64_map).fillna(1).astype('int8')\n",
    "    \n",
    "    CATS = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_66', 'D_68']\n",
    "    OFFSETS = [2,1,2,2,3,2,3,2,2] #2 minus minimal value in full train csv\n",
    "    # then 0 will be padding, 1 will be NAN, 2,3,4,etc will be values\n",
    "    for c,s in zip(CATS,OFFSETS):\n",
    "        train[c] = train[c] + s\n",
    "        train[c] = train[c].fillna(1).astype('int8')\n",
    "    CATS += ['D_63','D_64']\n",
    "    \n",
    "    # ADD NEW FEATURES HERE\n",
    "    # EXAMPLE: train['feature_189'] = etc etc etc\n",
    "    # EXAMPLE: train['feature_190'] = etc etc etc\n",
    "    # IF CATEGORICAL, THEN ADD TO CATS WITH: CATS += ['feaure_190'] etc etc etc\n",
    "    \n",
    "    # REDUCE MEMORY DTYPE\n",
    "    SKIP = ['customer_ID','year','month','day']\n",
    "    for c in train.columns:\n",
    "        if c in SKIP: continue\n",
    "        if str( train[c].dtype )=='int64':\n",
    "            train[c] = train[c].astype('int32')\n",
    "        if str( train[c].dtype )=='float64':\n",
    "            train[c] = train[c].astype('float32')\n",
    "            \n",
    "    # PAD ROWS SO EACH CUSTOMER HAS 13 ROWS\n",
    "    if PAD_CUSTOMER_TO_13_ROWS:\n",
    "        tmp = train[['customer_ID']].groupby('customer_ID').customer_ID.agg('count')\n",
    "        more = cupy.array([],dtype='int64') \n",
    "        for j in range(1,13):\n",
    "            i = tmp.loc[tmp==j].index.values\n",
    "            more = cupy.concatenate([more,cupy.repeat(i,13-j)])\n",
    "        df = train.iloc[:len(more)].copy().fillna(0)\n",
    "        df = df * 0 - 1 #pad numerical columns with -1\n",
    "        df[CATS] = (df[CATS] * 0).astype('int8') #pad categorical columns with 0\n",
    "        df['customer_ID'] = more\n",
    "        train = cudf.concat([train,df],axis=0,ignore_index=True)\n",
    "        \n",
    "    # ADD TARGETS (and reduce to 1 byte)\n",
    "    if targets is not None:\n",
    "        train = train.merge(targets,on='customer_ID',how='left')\n",
    "        train.target = train.target.astype('int8')\n",
    "        \n",
    "    # FILL NAN\n",
    "    train = train.fillna(-0.5) #this applies to numerical columns\n",
    "    \n",
    "    # SORT BY CUSTOMER THEN DATE\n",
    "    train = train.sort_values(['customer_ID','year','month','day']).reset_index(drop=True)\n",
    "    train = train.drop(['year','month','day'],axis=1)\n",
    "    \n",
    "    # REARRANGE COLUMNS WITH 11 CATS FIRST\n",
    "    COLS = list(train.columns[1:])\n",
    "    COLS = ['customer_ID'] + CATS + [c for c in COLS if c not in CATS]\n",
    "    train = train[COLS]\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d54fb8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T11:13:07.092033Z",
     "iopub.status.busy": "2022-08-09T11:13:07.091686Z",
     "iopub.status.idle": "2022-08-09T11:20:12.121732Z",
     "shell.execute_reply": "2022-08-09T11:20:12.120746Z"
    },
    "papermill": {
     "duration": 425.042352,
     "end_time": "2022-08-09T11:20:12.127191",
     "exception": false,
     "start_time": "2022-08-09T11:13:07.084839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_File_1 has 45891 customers and shape (596583, 190)\n",
      "Train_File_2 has 45891 customers and shape (596583, 190)\n",
      "Train_File_3 has 45891 customers and shape (596583, 190)\n",
      "Train_File_4 has 45891 customers and shape (596583, 190)\n",
      "Train_File_5 has 45891 customers and shape (596583, 190)\n",
      "Train_File_6 has 45891 customers and shape (596583, 190)\n",
      "Train_File_7 has 45891 customers and shape (596583, 190)\n",
      "Train_File_8 has 45891 customers and shape (596583, 190)\n",
      "Train_File_9 has 45891 customers and shape (596583, 190)\n",
      "Train_File_10 has 45894 customers and shape (596622, 190)\n"
     ]
    }
   ],
   "source": [
    "if PROCESS_DATA:\n",
    "    # CREATE PROCESSED TRAIN FILES AND SAVE TO DISK        \n",
    "    for k in range(NUM_FILES):\n",
    "\n",
    "        # READ CHUNK OF TRAIN CSV FILE\n",
    "        skip = int(np.sum( rows[:k] ) + 1) #the plus one is for skipping header\n",
    "        train = cudf.read_csv('../input/amex-default-prediction/train_data.csv', nrows=rows[k], \n",
    "                              skiprows=skip, header=None, names=T_COLS)\n",
    "\n",
    "        # FEATURE ENGINEER DATAFRAME\n",
    "        train = feature_engineering(train, targets = targets)\n",
    "\n",
    "        # SAVE FILES\n",
    "        print(f'Train_File_{k+1} has {train.customer_ID.nunique()} customers and shape',train.shape)\n",
    "        tar = train[['customer_ID','target']].drop_duplicates().sort_index()\n",
    "        if not os.path.exists(PATH_TO_DATA): os.makedirs(PATH_TO_DATA)\n",
    "        tar.to_parquet(f'{PATH_TO_DATA}targets_{k+1}.pqt',index=False)\n",
    "        data = train.iloc[:,1:-1].values.reshape((-1,13,188))\n",
    "        cupy.save(f'{PATH_TO_DATA}data_{k+1}',data.astype('float32'))\n",
    "\n",
    "    # CLEAN MEMORY\n",
    "    del train, tar, data\n",
    "    del targets\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb89956",
   "metadata": {
    "papermill": {
     "duration": 0.006062,
     "end_time": "2022-08-09T11:20:12.139789",
     "exception": false,
     "start_time": "2022-08-09T11:20:12.133727",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## AMEX metric code for Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45033a10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T11:20:12.152110Z",
     "iopub.status.busy": "2022-08-09T11:20:12.151808Z",
     "iopub.status.idle": "2022-08-09T11:20:12.165966Z",
     "shell.execute_reply": "2022-08-09T11:20:12.164972Z"
    },
    "papermill": {
     "duration": 0.022851,
     "end_time": "2022-08-09T11:20:12.168097",
     "exception": false,
     "start_time": "2022-08-09T11:20:12.145246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# COMPETITION METRIC FROM Konstantin Yakovlev\n",
    "# https://www.kaggle.com/kyakovlev\n",
    "# https://www.kaggle.com/competitions/amex-default-prediction/discussion/327534\n",
    "def amex_metric_mod(y_true, y_pred):\n",
    "\n",
    "    labels     = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels     = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights    = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals   = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four   = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels         = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels         = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight         = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random  = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos      = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found  = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz        = cum_pos_found / total_pos\n",
    "        gini[i]        = np.sum((lorentz - weight_random) * weight)\n",
    "\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ef1619",
   "metadata": {
    "papermill": {
     "duration": 0.005924,
     "end_time": "2022-08-09T11:20:12.179774",
     "exception": false,
     "start_time": "2022-08-09T11:20:12.173850",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2574f550",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T11:20:12.192131Z",
     "iopub.status.busy": "2022-08-09T11:20:12.191858Z",
     "iopub.status.idle": "2022-08-09T11:20:12.200275Z",
     "shell.execute_reply": "2022-08-09T11:20:12.199443Z"
    },
    "papermill": {
     "duration": 0.017085,
     "end_time": "2022-08-09T11:20:12.202376",
     "exception": false,
     "start_time": "2022-08-09T11:20:12.185291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gated Recurrent Unit Model, Find more on the link below\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU\n",
    "\n",
    "def build_model():\n",
    "    \n",
    "    # INPUT - FIRST 11 COLUMNS ARE CAT, NEXT 177 ARE NUMERIC\n",
    "    inp = tf.keras.Input(shape=(13,188))\n",
    "    embeddings = []\n",
    "    for k in range(11):\n",
    "        emb = tf.keras.layers.Embedding(10, 4, embeddings_initializer='uniform',\n",
    "                                        embeddings_regularizer=None, \n",
    "                                        activity_regularizer=None, \n",
    "                                        embeddings_constraint=None, \n",
    "                                        mask_zero=False, \n",
    "                                        input_length=None)\n",
    "        embeddings.append(emb(inp[:,:,k]))\n",
    "\n",
    "    x = tf.keras.layers.Concatenate()([inp[:,:,11:]]+embeddings)\n",
    "    \n",
    "    # SIMPLE RNN BACKBONE\n",
    "    x = tf.keras.layers.GRU(units=128, return_sequences=False)(x)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
    "    \n",
    "    # OUTPUT\n",
    "    x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    # COMPILE MODEL\n",
    "    model = tf.keras.Model(inputs=inp, outputs=x)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    model.compile(loss=loss, optimizer = opt)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd70a46",
   "metadata": {
    "papermill": {
     "duration": 0.005826,
     "end_time": "2022-08-09T11:20:12.213680",
     "exception": false,
     "start_time": "2022-08-09T11:20:12.207854",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffc6b66d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T11:20:12.225858Z",
     "iopub.status.busy": "2022-08-09T11:20:12.225599Z",
     "iopub.status.idle": "2022-08-09T11:29:38.887818Z",
     "shell.execute_reply": "2022-08-09T11:29:38.886797Z"
    },
    "papermill": {
     "duration": 566.672753,
     "end_time": "2022-08-09T11:29:38.891930",
     "exception": false,
     "start_time": "2022-08-09T11:20:12.219177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### Fold 1 with valid files [1, 2]\n",
      "### Training data shapes (367131, 13, 188) (367131,)\n",
      "### Validation data shapes (91782, 13, 188) (91782,)\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-09 11:20:41.735131: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 3589072656 exceeds 10% of free system memory.\n",
      "2022-08-09 11:20:46.007281: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 3589072656 exceeds 10% of free system memory.\n",
      "2022-08-09 11:20:49.081957: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-09 11:20:52.598127: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n",
      "2022-08-09 11:21:02.928017: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 897260832 exceeds 10% of free system memory.\n",
      "2022-08-09 11:21:04.221465: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 897260832 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "718/718 - 17s - loss: 0.2406 - val_loss: 0.2331\n",
      "Epoch 2/8\n",
      "718/718 - 9s - loss: 0.2271 - val_loss: 0.2292\n",
      "Epoch 3/8\n",
      "718/718 - 9s - loss: 0.2239 - val_loss: 0.2298\n",
      "Epoch 4/8\n",
      "718/718 - 9s - loss: 0.2213 - val_loss: 0.2267\n",
      "Epoch 5/8\n",
      "718/718 - 10s - loss: 0.2191 - val_loss: 0.2255\n",
      "Epoch 6/8\n",
      "718/718 - 9s - loss: 0.2172 - val_loss: 0.2268\n",
      "Epoch 7/8\n",
      "718/718 - 9s - loss: 0.2152 - val_loss: 0.2259\n",
      "Epoch 8/8\n",
      "718/718 - 9s - loss: 0.2126 - val_loss: 0.2262\n",
      "Inferring validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-09 11:22:11.044883: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 897260832 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 - 1s\n",
      "\n",
      "Fold 1 CV= 0.7841543630128613\n",
      "\n",
      "#########################\n",
      "### Fold 2 with valid files [3, 4]\n",
      "### Training data shapes (367131, 13, 188) (367131,)\n",
      "### Validation data shapes (91782, 13, 188) (91782,)\n",
      "#########################\n",
      "Epoch 1/8\n",
      "718/718 - 13s - loss: 0.2378 - val_loss: 0.2366\n",
      "Epoch 2/8\n",
      "718/718 - 9s - loss: 0.2266 - val_loss: 0.2379\n",
      "Epoch 3/8\n",
      "718/718 - 10s - loss: 0.2236 - val_loss: 0.2271\n",
      "Epoch 4/8\n",
      "718/718 - 9s - loss: 0.2206 - val_loss: 0.2298\n",
      "Epoch 5/8\n",
      "718/718 - 9s - loss: 0.2187 - val_loss: 0.2248\n",
      "Epoch 6/8\n",
      "718/718 - 9s - loss: 0.2164 - val_loss: 0.2248\n",
      "Epoch 7/8\n",
      "718/718 - 9s - loss: 0.2144 - val_loss: 0.2243\n",
      "Epoch 8/8\n",
      "718/718 - 10s - loss: 0.2118 - val_loss: 0.2261\n",
      "Inferring validation data...\n",
      "180/180 - 1s\n",
      "\n",
      "Fold 2 CV= 0.7792243451201806\n",
      "\n",
      "#########################\n",
      "### Fold 3 with valid files [5, 6]\n",
      "### Training data shapes (367131, 13, 188) (367131,)\n",
      "### Validation data shapes (91782, 13, 188) (91782,)\n",
      "#########################\n",
      "Epoch 1/8\n",
      "718/718 - 14s - loss: 0.2419 - val_loss: 0.2304\n",
      "Epoch 2/8\n",
      "718/718 - 9s - loss: 0.2274 - val_loss: 0.2270\n",
      "Epoch 3/8\n",
      "718/718 - 9s - loss: 0.2239 - val_loss: 0.2252\n",
      "Epoch 4/8\n",
      "718/718 - 9s - loss: 0.2213 - val_loss: 0.2231\n",
      "Epoch 5/8\n",
      "718/718 - 9s - loss: 0.2192 - val_loss: 0.2228\n",
      "Epoch 6/8\n",
      "718/718 - 10s - loss: 0.2170 - val_loss: 0.2226\n",
      "Epoch 7/8\n",
      "718/718 - 9s - loss: 0.2150 - val_loss: 0.2252\n",
      "Epoch 8/8\n",
      "718/718 - 9s - loss: 0.2126 - val_loss: 0.2226\n",
      "Inferring validation data...\n",
      "180/180 - 1s\n",
      "\n",
      "Fold 3 CV= 0.7852663237446941\n",
      "\n",
      "#########################\n",
      "### Fold 4 with valid files [7, 8]\n",
      "### Training data shapes (367131, 13, 188) (367131,)\n",
      "### Validation data shapes (91782, 13, 188) (91782,)\n",
      "#########################\n",
      "Epoch 1/8\n",
      "718/718 - 13s - loss: 0.2384 - val_loss: 0.2286\n",
      "Epoch 2/8\n",
      "718/718 - 9s - loss: 0.2272 - val_loss: 0.2255\n",
      "Epoch 3/8\n",
      "718/718 - 9s - loss: 0.2244 - val_loss: 0.2238\n",
      "Epoch 4/8\n",
      "718/718 - 9s - loss: 0.2219 - val_loss: 0.2209\n",
      "Epoch 5/8\n",
      "718/718 - 9s - loss: 0.2198 - val_loss: 0.2310\n",
      "Epoch 6/8\n",
      "718/718 - 9s - loss: 0.2177 - val_loss: 0.2221\n",
      "Epoch 7/8\n",
      "718/718 - 9s - loss: 0.2156 - val_loss: 0.2274\n",
      "Epoch 8/8\n",
      "718/718 - 10s - loss: 0.2136 - val_loss: 0.2219\n",
      "Inferring validation data...\n",
      "180/180 - 1s\n",
      "\n",
      "Fold 4 CV= 0.7868950805982784\n",
      "\n",
      "#########################\n",
      "### Fold 5 with valid files [9, 10]\n",
      "### Training data shapes (367128, 13, 188) (367128,)\n",
      "### Validation data shapes (91785, 13, 188) (91785,)\n",
      "#########################\n",
      "Epoch 1/8\n",
      "718/718 - 14s - loss: 0.2403 - val_loss: 0.2265\n",
      "Epoch 2/8\n",
      "718/718 - 9s - loss: 0.2279 - val_loss: 0.2261\n",
      "Epoch 3/8\n",
      "718/718 - 9s - loss: 0.2247 - val_loss: 0.2239\n",
      "Epoch 4/8\n",
      "718/718 - 10s - loss: 0.2222 - val_loss: 0.2216\n",
      "Epoch 5/8\n",
      "718/718 - 9s - loss: 0.2201 - val_loss: 0.2205\n",
      "Epoch 6/8\n",
      "718/718 - 9s - loss: 0.2180 - val_loss: 0.2206\n",
      "Epoch 7/8\n",
      "718/718 - 10s - loss: 0.2158 - val_loss: 0.2246\n",
      "Epoch 8/8\n",
      "718/718 - 9s - loss: 0.2135 - val_loss: 0.2205\n",
      "Inferring validation data...\n",
      "180/180 - 1s\n",
      "\n",
      "Fold 5 CV= 0.7876141639767068\n",
      "\n",
      "#########################\n",
      "Overall CV = 0.7836767429166765\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_MODEL:\n",
    "    # SAVE TRUE AND OOF\n",
    "    true = np.array([])\n",
    "    oof = np.array([])\n",
    "    VERBOSE = 2 # use 1 for interactive \n",
    "\n",
    "    for fold in range(5):\n",
    "\n",
    "        # INDICES OF TRAIN AND VALID FOLDS\n",
    "        valid_idx = [2*fold+1, 2*fold+2]\n",
    "        train_idx = [x for x in [1,2,3,4,5,6,7,8,9,10] if x not in valid_idx]\n",
    "\n",
    "        print('#'*25)\n",
    "        print(f'### Fold {fold+1} with valid files', valid_idx)\n",
    "\n",
    "        # READ TRAIN DATA FROM DISK\n",
    "        X_train = []; y_train = []\n",
    "        for k in train_idx:\n",
    "            X_train.append( np.load(f'{PATH_TO_DATA}data_{k}.npy'))\n",
    "            y_train.append( pd.read_parquet(f'{PATH_TO_DATA}targets_{k}.pqt') )\n",
    "        X_train = np.concatenate(X_train,axis=0)\n",
    "        y_train = pd.concat(y_train).target.values\n",
    "        print('### Training data shapes', X_train.shape, y_train.shape)\n",
    "\n",
    "        # READ VALID DATA FROM DISK\n",
    "        X_valid = []; y_valid = []\n",
    "        for k in valid_idx:\n",
    "            X_valid.append( np.load(f'{PATH_TO_DATA}data_{k}.npy'))\n",
    "            y_valid.append( pd.read_parquet(f'{PATH_TO_DATA}targets_{k}.pqt') )\n",
    "        X_valid = np.concatenate(X_valid,axis=0)\n",
    "        y_valid = pd.concat(y_valid).target.values\n",
    "        print('### Validation data shapes', X_valid.shape, y_valid.shape)\n",
    "        print('#'*25)\n",
    "\n",
    "        # BUILD AND TRAIN MODEL\n",
    "        K.clear_session()\n",
    "        model = build_model()\n",
    "        h = model.fit(X_train,y_train, \n",
    "                      validation_data = (X_valid,y_valid),\n",
    "                      batch_size=512, epochs=8, verbose=VERBOSE)\n",
    "        if not os.path.exists(PATH_TO_MODEL): os.makedirs(PATH_TO_MODEL)\n",
    "        model.save_weights(f'{PATH_TO_MODEL}gru_fold_{fold+1}.h5')\n",
    "\n",
    "        # INFER VALID DATA\n",
    "        print('Inferring validation data...')\n",
    "        p = model.predict(X_valid, batch_size=512, verbose=VERBOSE).flatten()\n",
    "\n",
    "        print()\n",
    "        print(f'Fold {fold+1} CV=', amex_metric_mod(y_valid, p) )\n",
    "        print()\n",
    "        true = np.concatenate([true, y_valid])\n",
    "        oof = np.concatenate([oof, p])\n",
    "        \n",
    "        # CLEAN MEMORY\n",
    "        del model, X_train, y_train, X_valid, y_valid, p\n",
    "        gc.collect()\n",
    "\n",
    "    # PRINT OVERALL RESULTS\n",
    "    print('#'*25)\n",
    "    print(f'Overall CV =', amex_metric_mod(true, oof) )\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042fe22d",
   "metadata": {
    "papermill": {
     "duration": 0.011596,
     "end_time": "2022-08-09T11:29:38.916176",
     "exception": false,
     "start_time": "2022-08-09T11:29:38.904580",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Process Test DataÂ¶\n",
    "We'll process test data same as we processed train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0de0032",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T11:29:38.941292Z",
     "iopub.status.busy": "2022-08-09T11:29:38.940355Z",
     "iopub.status.idle": "2022-08-09T11:36:42.581441Z",
     "shell.execute_reply": "2022-08-09T11:36:42.580401Z"
    },
    "papermill": {
     "duration": 423.669562,
     "end_time": "2022-08-09T11:36:42.596834",
     "exception": false,
     "start_time": "2022-08-09T11:29:38.927272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 190 test dataframe columns\n",
      "There are 924621 unique customers in test.\n"
     ]
    }
   ],
   "source": [
    "if PROCESS_DATA:\n",
    "    # GET TEST COLUMN NAMES\n",
    "    test = cudf.read_csv('../input/amex-default-prediction/test_data.csv', nrows=1)\n",
    "    T_COLS = test.columns\n",
    "    print(f'There are {len(T_COLS)} test dataframe columns')\n",
    "    \n",
    "    # GET TEST CUSTOMER NAMES (use pandas to avoid memory error)\n",
    "    test = pd.read_csv('../input/amex-default-prediction/test_data.csv', usecols=['customer_ID'])\n",
    "    test['customer_ID'] = test['customer_ID'].apply(lambda x: int(x[-16:],16) ).astype('int64')\n",
    "    \n",
    "    customers = test.drop_duplicates().sort_index().values.flatten()\n",
    "    print(f'There are {len(customers)} unique customers in test.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75109f75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T11:36:42.624059Z",
     "iopub.status.busy": "2022-08-09T11:36:42.623706Z",
     "iopub.status.idle": "2022-08-09T11:36:44.820671Z",
     "shell.execute_reply": "2022-08-09T11:36:44.819447Z"
    },
    "papermill": {
     "duration": 2.213946,
     "end_time": "2022-08-09T11:36:44.823414",
     "exception": false,
     "start_time": "2022-08-09T11:36:42.609468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will split test data into 20 separate files.\n",
      "There will be 46231 customers in each file (except the last file).\n",
      "Below are number of rows in each file:\n",
      "[567933, 568482, 569369, 567886, 567539, 568041, 568138, 567596, 568543, 567539, 568421, 568745, 568279, 568333, 568327, 568901, 568300, 568001, 567372, 568017]\n"
     ]
    }
   ],
   "source": [
    "NUM_FILES = 20\n",
    "if PROCESS_DATA:\n",
    "    # Calculate size of each separate file\n",
    "    rows = get_rows(customers, test, NUM_FILES = NUM_FILES, verbose = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68630636",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T11:36:44.848883Z",
     "iopub.status.busy": "2022-08-09T11:36:44.848532Z",
     "iopub.status.idle": "2022-08-09T12:35:09.300346Z",
     "shell.execute_reply": "2022-08-09T12:35:09.299247Z"
    },
    "papermill": {
     "duration": 3504.468166,
     "end_time": "2022-08-09T12:35:09.304025",
     "exception": false,
     "start_time": "2022-08-09T11:36:44.835859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_File_1 has 46231 customers and shape (601003, 189)\n",
      "Test_File_2 has 46231 customers and shape (601003, 189)\n",
      "Test_File_3 has 46231 customers and shape (601003, 189)\n",
      "Test_File_4 has 46231 customers and shape (601003, 189)\n",
      "Test_File_5 has 46231 customers and shape (601003, 189)\n",
      "Test_File_6 has 46231 customers and shape (601003, 189)\n",
      "Test_File_7 has 46231 customers and shape (601003, 189)\n",
      "Test_File_8 has 46231 customers and shape (601003, 189)\n",
      "Test_File_9 has 46231 customers and shape (601003, 189)\n",
      "Test_File_10 has 46231 customers and shape (601003, 189)\n",
      "Test_File_11 has 46231 customers and shape (601003, 189)\n",
      "Test_File_12 has 46231 customers and shape (601003, 189)\n",
      "Test_File_13 has 46231 customers and shape (601003, 189)\n",
      "Test_File_14 has 46231 customers and shape (601003, 189)\n",
      "Test_File_15 has 46231 customers and shape (601003, 189)\n",
      "Test_File_16 has 46231 customers and shape (601003, 189)\n",
      "Test_File_17 has 46231 customers and shape (601003, 189)\n",
      "Test_File_18 has 46231 customers and shape (601003, 189)\n",
      "Test_File_19 has 46231 customers and shape (601003, 189)\n",
      "Test_File_20 has 46232 customers and shape (601016, 189)\n"
     ]
    }
   ],
   "source": [
    "if PROCESS_DATA:\n",
    "    # SAVE TEST CUSTOMERS INDEX\n",
    "    test_customer_hashes = cupy.array([],dtype='int64')\n",
    "    \n",
    "    # CREATE PROCESSED TEST FILES AND SAVE TO DISK\n",
    "    for k in range(NUM_FILES):\n",
    "\n",
    "        # READ CHUNK OF TEST CSV FILE\n",
    "        skip = int(np.sum( rows[:k] ) + 1) #the plus one is for skipping header\n",
    "        test = cudf.read_csv('/kaggle/input/amex-default-prediction/test_data.csv', nrows=rows[k], \n",
    "                              skiprows=skip, header=None, names=T_COLS)\n",
    "\n",
    "        # FEATURE ENGINEER DATAFRAME\n",
    "        test = feature_engineering(test, targets = None)\n",
    "        \n",
    "        # SAVE TEST CUSTOMERS INDEX\n",
    "        cust = test[['customer_ID']].drop_duplicates().sort_index().values.flatten()\n",
    "        test_customer_hashes = cupy.concatenate([test_customer_hashes,cust])\n",
    "\n",
    "        # SAVE FILES\n",
    "        print(f'Test_File_{k+1} has {test.customer_ID.nunique()} customers and shape',test.shape)\n",
    "        data = test.iloc[:,1:].values.reshape((-1,13,188))\n",
    "        cupy.save(f'{PATH_TO_DATA}test_data_{k+1}',data.astype('float32'))\n",
    "        \n",
    "    # SAVE CUSTOMER INDEX OF ALL TEST FILES\n",
    "    cupy.save(f'{PATH_TO_DATA}test_hashes_data', test_customer_hashes)\n",
    "\n",
    "    # CLEAN MEMORY\n",
    "    del test, data\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0bed37",
   "metadata": {
    "papermill": {
     "duration": 0.01234,
     "end_time": "2022-08-09T12:35:09.329149",
     "exception": false,
     "start_time": "2022-08-09T12:35:09.316809",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Infer test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4acece80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T12:35:09.356422Z",
     "iopub.status.busy": "2022-08-09T12:35:09.356037Z",
     "iopub.status.idle": "2022-08-09T12:38:24.199284Z",
     "shell.execute_reply": "2022-08-09T12:38:24.198264Z"
    },
    "papermill": {
     "duration": 194.860754,
     "end_time": "2022-08-09T12:38:24.202954",
     "exception": false,
     "start_time": "2022-08-09T12:35:09.342200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferring Test_File_1\n",
      "Inferring Test_File_2\n",
      "Inferring Test_File_3\n",
      "Inferring Test_File_4\n",
      "Inferring Test_File_5\n",
      "Inferring Test_File_6\n",
      "Inferring Test_File_7\n",
      "Inferring Test_File_8\n",
      "Inferring Test_File_9\n",
      "Inferring Test_File_10\n",
      "Inferring Test_File_11\n",
      "Inferring Test_File_12\n",
      "Inferring Test_File_13\n",
      "Inferring Test_File_14\n",
      "Inferring Test_File_15\n",
      "Inferring Test_File_16\n",
      "Inferring Test_File_17\n",
      "Inferring Test_File_18\n",
      "Inferring Test_File_19\n",
      "Inferring Test_File_20\n"
     ]
    }
   ],
   "source": [
    "if INFER_TEST:\n",
    "    # INFER TEST DATA\n",
    "    start = 0; end = 0\n",
    "    sub = cudf.read_csv('/kaggle/input/amex-default-prediction/sample_submission.csv')\n",
    "    \n",
    "    # REARANGE SUB ROWS TO MATCH PROCESSED TEST FILES\n",
    "    sub['hash'] = sub['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n",
    "    test_hash_index = cupy.load(f'{PATH_TO_DATA}test_hashes_data.npy')\n",
    "    sub = sub.set_index('hash').loc[test_hash_index].reset_index(drop=True)\n",
    "    \n",
    "    for k in range(NUM_FILES):\n",
    "        # BUILD MODEL\n",
    "        K.clear_session()\n",
    "        model = build_model()\n",
    "        \n",
    "        # LOAD TEST DATA\n",
    "        print(f'Inferring Test_File_{k+1}')\n",
    "        X_test = np.load(f'{PATH_TO_DATA}test_data_{k+1}.npy')\n",
    "        end = start + X_test.shape[0]\n",
    "\n",
    "        # INFER 5 FOLD MODELS\n",
    "        model.load_weights(f'{PATH_TO_MODEL}gru_fold_1.h5')\n",
    "        p = model.predict(X_test, batch_size=512, verbose=0).flatten() \n",
    "        for j in range(1,5):\n",
    "            model.load_weights(f'{PATH_TO_MODEL}gru_fold_{j+1}.h5')\n",
    "            p += model.predict(X_test, batch_size=512, verbose=0).flatten()\n",
    "        p /= 5.0\n",
    "\n",
    "        # SAVE TEST PREDICTIONS\n",
    "        sub.loc[start:end-1,'prediction'] = p\n",
    "        start = end\n",
    "        \n",
    "        # CLEAN MEMORY\n",
    "        del model, X_test, p\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8412cfd",
   "metadata": {
    "papermill": {
     "duration": 0.02767,
     "end_time": "2022-08-09T12:38:24.271135",
     "exception": false,
     "start_time": "2022-08-09T12:38:24.243465",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "348d02b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T12:38:24.315799Z",
     "iopub.status.busy": "2022-08-09T12:38:24.314848Z",
     "iopub.status.idle": "2022-08-09T12:38:24.589850Z",
     "shell.execute_reply": "2022-08-09T12:38:24.588873Z"
    },
    "papermill": {
     "duration": 0.300039,
     "end_time": "2022-08-09T12:38:24.591976",
     "exception": false,
     "start_time": "2022-08-09T12:38:24.291937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file shape is (924621, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>038be0571bd6b3776cb8512731968f4de302c811030124...</td>\n",
       "      <td>0.003108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0074a0233ef766b52884608cc8cf9098f59d885b5d59fc...</td>\n",
       "      <td>0.000215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>060b8b7f30f795a0e93995d45b29461ffa6ece0eeb5c3d...</td>\n",
       "      <td>0.093942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>03a1d125bdd776000bf0b28238d0bea240ad581d332e70...</td>\n",
       "      <td>0.153480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0290f245dd35ba899af52316ccc62b2627e7ae18cd76a2...</td>\n",
       "      <td>0.277354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_ID  prediction\n",
       "0  038be0571bd6b3776cb8512731968f4de302c811030124...    0.003108\n",
       "1  0074a0233ef766b52884608cc8cf9098f59d885b5d59fc...    0.000215\n",
       "2  060b8b7f30f795a0e93995d45b29461ffa6ece0eeb5c3d...    0.093942\n",
       "3  03a1d125bdd776000bf0b28238d0bea240ad581d332e70...    0.153480\n",
       "4  0290f245dd35ba899af52316ccc62b2627e7ae18cd76a2...    0.277354"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if INFER_TEST:\n",
    "    sub.to_csv('submission.csv',index=False)\n",
    "    print('Submission file shape is', sub.shape )\n",
    "    display(sub.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5341.579579,
   "end_time": "2022-08-09T12:38:27.543543",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-09T11:09:25.963964",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
